{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu74w6bTW9v-",
        "colab_type": "text"
      },
      "source": [
        "# Laboratorio de estimación no-paramétrica\n",
        "Procesamiento Adaptativo de Señales Aleatorias\n",
        "\n",
        "© 2019 Tomas A. González Orlando\n",
        "\n",
        "© 2019 Lucero G. Fernandez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfWTbsZLZrSq",
        "colab_type": "text"
      },
      "source": [
        "# Ejercicio 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KucEcksBv_R",
        "colab_type": "text"
      },
      "source": [
        "## Preparativos\n",
        "Debe guardarse el archivo readligo.py en el directorio correspondiente para poder leer los datos correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syyyeisyJMD0",
        "colab_type": "text"
      },
      "source": [
        "### Clonamos el repositorio de Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dLRkh_BIwfM",
        "colab_type": "code",
        "outputId": "a7a4faff-bc6d-408f-f10b-2fd7bc73bb12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "#https://medium.com/@navan0/how-to-push-files-into-github-from-google-colab-379fd0077aa8\n",
        "!git clone https://github.com/LuFernandez/PASA.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PASA'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 564 (delta 13), reused 20 (delta 3), pack-reused 520\u001b[K\n",
            "Receiving objects: 100% (564/564), 83.79 MiB | 33.03 MiB/s, done.\n",
            "Resolving deltas: 100% (267/267), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokwgkdZKEKZ",
        "colab_type": "code",
        "outputId": "442b2aff-23e7-48bf-e34e-c963049c3bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!git init\n",
        "!git config -- global user.email “togonzalez@itba.edu.ar”\n",
        "!git config -- global user.name “taomasgonzalez”"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized empty Git repository in /content/.git/\n",
            "error: key does not contain a section: global\n",
            "error: key does not contain a section: global\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2A6Vx1wJVAu",
        "colab_type": "text"
      },
      "source": [
        "### Imports necesarios a lo largo del ejercicio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoSjWtSZ93Oi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "6698ae77-b22a-4415-ba47-ee60d93c21b3"
      },
      "source": [
        "import h5py as h5\n",
        "\n",
        "import scipy\n",
        "from scipy import signal as scsig\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy import fftpack as scfft\n",
        "\n",
        "import numpy as np\n",
        "from numpy.lib import scimath\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import readligo as rl\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "import matplotlib.mlab as mlab"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5b3c4ddbdd8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mreadligo\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'readligo'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P68c0QA9-JI",
        "colab_type": "text"
      },
      "source": [
        "## Lectura de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7DfiI3EZSSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(path, name):\n",
        "  strain, time, chan_dict = rl.loaddata(path, name)\n",
        "  dt = time[1] - time[0]\n",
        "  times = dt*np.array([i for i in range(len(strain))])\n",
        "  return (strain, times, chan_dict, dt)\n",
        "\n",
        "fs = 4096\n",
        "folder_path = \"PASA/LAB1/LIGO/\"\n",
        "# leemos los datos de Livingston\n",
        "strain_l1, time_l1, chan_dict_l1, dt_l1 = read_data(path=folder_path+'L-L1_LOSC_4_V2-1126259446-32.hdf5', name='L1')\n",
        "# leemos los datos de Hanford\n",
        "strain_h1, time_h1, chan_dict_h1, dt_h1 = read_data(path=folder_path+'H-H1_LOSC_4_V2-1126259446-32.hdf5', name='H1')\n",
        "# strain_h1, time_h1, chan_dict_h1 = rl.loaddata('H-H1_LOSC_4_V2-1126259446-32.hdf5', 'L1')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixw6IHWsB5b4",
        "colab_type": "text"
      },
      "source": [
        "## Visualización de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkJZbDW_B7st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(time_l1, strain_h1)\n",
        "plt.plot(time_h1, strain_l1)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoLK2-ota0hl",
        "colab_type": "text"
      },
      "source": [
        "## a) Estimación del espectro de potencia del ruido mediante el uso de periodogramas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRWU-EWhBfa7",
        "colab_type": "text"
      },
      "source": [
        "### Implementación del periodograma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4B-mdLBWxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def periodogram(fs, data, figure_number, x_lim, title, plot=True):\n",
        "  # cantidad de muestras\n",
        "  N = len(data)\n",
        "  z = scfft.fft(x=data, n=N)\n",
        "  R = z*np.conj(z)\n",
        "  freqs = np.fft.fftfreq(N)\n",
        "  xf = np.linspace(0, fs/2, N//2)    #frecuencias\n",
        "  if plot:\n",
        "    plt.loglog(xf, 2/N * np.abs(R[:N//2]))\n",
        "    plt.xlim(x_lim)\n",
        "    plt.grid()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Frequency (Hz)\")\n",
        "    plt.ylabel(\"Power Spectral Density Rx(f)\")\n",
        "  \n",
        "  return (xf, R[:N//2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tpcD_NMBSH9",
        "colab_type": "text"
      },
      "source": [
        "### Aplicado a Livingston\n",
        "Hacemos el periodograma para los datos de Livingston en su totalidad, y divididimos en sectores para calcular el error y corroborar estacionariedad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co0ATZ7VBddO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_f_l1_tot, p_Pxx_l1_tot = periodogram(fs=fs, data=strain_l1, figure_number=1, x_lim=(10, 2000), title=\"Periodograma L1\")\n",
        "plt.show()\n",
        "p_f_l1_first, p_Pxx_l1_first = periodogram(fs=fs, data=strain_l1[:len(strain_l1)//3], figure_number=1, x_lim=(10, 2000), title=\"Periodograma L1, primeros 10 segundos\")\n",
        "plt.show()\n",
        "p_f_l1_last, p_Pxx_l1_last = periodogram(fs=fs, data=strain_l1[int(len(strain_l1)*2/3):], figure_number=1, x_lim=(10, 2000), title=\"Periodograma L1, últimos 10 segundos\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PazN5w0zQMv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " rmse = np.square(np.abs(np.subtract(p_Pxx_l1_first,p_Pxx_l1_last))).mean() \n",
        " print(\"La raíz del error cuadrático medio o RMSE (Rooted Mean Square Error) es:\", rmse,\n",
        "       \"\\npor lo que se puede decir que el periodograma de un tramo de los datos es representativo de todo el ensamble.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2onWiBaCPQj",
        "colab_type": "text"
      },
      "source": [
        "### Aplicado a Hanford\n",
        "Hacemos el periodograma para los datos de Hanford en su totalidad, y divididimos en secotres para calcular el error y corroborar estacionariedad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5PXg1PHCyfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_f_h1_tot, Pxx_h1_tot = periodogram(fs=fs, data=strain_h1, figure_number=1, x_lim=(10, 2000), title=\"Periodograma H1\")\n",
        "plt.show()\n",
        "p_f_h1_first, p_Pxx_h1_first = periodogram(fs=fs, data=strain_h1[:len(strain_h1)//3], figure_number=2, x_lim=(10, 2000), title=\"Periodograma H1, primeros 10 segundos\")\n",
        "plt.show()\n",
        "p_f_h1_last, p_Pxx_h1_last = periodogram(fs=fs, data=strain_h1[2*len(strain_l1)//3:], figure_number=3, x_lim=(10, 2000), title=\"Periodograma H1, últimos 10 segundos\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFY3bczRaTgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rmse = np.square(np.abs(np.subtract(p_Pxx_h1_first,p_Pxx_h1_last))).mean() \n",
        "print(\"La raíz del error cuadrático medio o RMSE (Rooted Mean Square Error) es:\", rmse,\n",
        "      \"\\npor lo que se puede decir que el periodograma de un tramo de los datos es representativo de todo el ensamble.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqGJeiK-X-W4",
        "colab_type": "text"
      },
      "source": [
        "###Superponemos gráficos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTUPehHfX00w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_f_h1_first, p_Pxx_h1_first = periodogram(fs=fs, data=strain_h1[:len(strain_h1)//3], figure_number=2, x_lim=(10, 2000), title=\"Periodograma H1, primeros 10 segundos\")\n",
        "p_f_l1_first, p_Pxx_l1_first = periodogram(fs=fs, data=strain_l1[:len(strain_l1)//3], figure_number=1, x_lim=(10, 2000), title=\"Periodograma L1, primeros 10 segundos\")\n",
        "plt.legend(['H1','L1'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qGpZiB9HWMy",
        "colab_type": "text"
      },
      "source": [
        "## b) Estimación del espectro de potencia del ruido mediante el uso del método de Blackman-Tukey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JRjgmeNHx2v",
        "colab_type": "text"
      },
      "source": [
        "### Implementación del método de Blackman-Tukey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-bSBXVrkfH4",
        "colab_type": "text"
      },
      "source": [
        "Definimos cómo crear la ventana necesaria:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyzn-t6ikjJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_b_tukey_window(N, M, L):\n",
        "  delta_w = 2*np.pi/N* (2*M+1) \n",
        "  l = np.arange(1,L+1)\n",
        "  window = np.sin(delta_w*l / 2)  / (np.pi * l)                 \n",
        "  window = np.append( np.flip(window), np.append([delta_w/2/np.pi] , window) )\n",
        "  return window"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5fBNvOwk5Lg",
        "colab_type": "text"
      },
      "source": [
        "Definimos cómo estimar la autocorrelación de una señal ruidosa:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlFTRDKQlCr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def autocorrelation_estimation(data, L, N):\n",
        "  # Formamos el vector r_x con todos l de 0 a L \n",
        "  if data.dtype is not complex:\n",
        "    data = data.astype(complex)\n",
        "    \n",
        "  # complex conjugate of the first argument is used for the calculation of the dot product.\n",
        "  r_x = np.array([np.vdot(data[0:N-l]/N, data[l:N]) for l in range(0, L+1)])\n",
        "  # terminamos de completar r_x con los valores de -L a -1:\n",
        "  r_x = np.append( np.flip( np.conj(r_x[1:]) ) , r_x )\n",
        "  return r_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnNxz3gUlrMn",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, implementamos el método:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjwDOz-gHPBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#frac_N_l = N/l\n",
        "def blackman_tukey(fs, data, figure_number, M, x_lim, title='', N=4096, frac_N_l=5, plot=True):\n",
        "  # cantidad de muestras N\n",
        "  # Se recomienda N mayor a 50, y |l| ≤ N/4 (Box & Jenkins, 1976). \n",
        "  L = N//frac_N_l\n",
        "  if N <= 50:\n",
        "    print('Se recomienda elegir una cantidad de muestras mayor a 50! (Box & Jenkins, 1976)')\n",
        "  if L > N//4:\n",
        "    print('Se recomienda elegir |l|<=N/4! (Box & Jenkins, 1976)')\n",
        "    \n",
        "  r_xw = autocorrelation_estimation(data=data, L=L, N=N) * create_b_tukey_window(N, M, L)\n",
        "\n",
        "  z = abs(scfft.fft(x=r_xw, n=len(r_xw)))\n",
        "  \n",
        "  freqs = np.fft.fftfreq(n=len(r_xw), d=1/fs)\n",
        "  if plot: \n",
        "    plt.loglog(freqs, z)\n",
        "    plt.xlim(x_lim)\n",
        "    plt.grid()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Frequency (Hz)\")\n",
        "    plt.ylabel(\"Power Spectral Density Rx(f)\")\n",
        "  return (freqs, z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG9NEHcKH274",
        "colab_type": "text"
      },
      "source": [
        "### Aplicado a los datos de Livingston"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DypTFIFEIBcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_f_l1_tot, b_Pxx_l1_tot = blackman_tukey(fs=fs, data=strain_l1, figure_number=1, x_lim=(10,2000), title='Strain L1', M=4, N=4096*10, frac_N_l=16)\n",
        "plt.show()\n",
        "b_f_l1_first, b_Pxx_l1_first = blackman_tukey(fs=fs, data=strain_l1[:len(strain_l1)//3], figure_number=1, x_lim=(10,2000), title='Strain L1', M=4, N=4096*3, frac_N_l=16)\n",
        "plt.show()\n",
        "b_f_l1_last, b_Pxx_l1_last = blackman_tukey(fs=fs, data=strain_l1[2*len(strain_l1)//3:], figure_number=1, x_lim=(10,2000), title='Strain L1', M=4, N=4096*3, frac_N_l=8)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht8Uwfi0H7Em",
        "colab_type": "text"
      },
      "source": [
        "### Aplicado a los datos de Hanford"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5eLt4t7IB2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_f_h1_tot, b_Pxx_h1_tot = blackman_tukey(fs=fs, data=strain_h1, figure_number=1, x_lim=(10,2000), title='Strain H1', M=20,N=4096*5, frac_N_l=5)\n",
        "plt.show()\n",
        "b_f_h1_first, b_Pxx_h1_first = blackman_tukey(fs=fs, data=strain_h1, figure_number=1, x_lim=(10,2000), title='Strain H1', M=20,N=4096*2, frac_N_l=5)\n",
        "plt.show()\n",
        "b_f_h1_last, b_Pxx_h1_last = blackman_tukey(fs=fs, data=strain_h1, figure_number=1, x_lim=(10,2000), title='Strain H1', M=20,N=4096*3, frac_N_l=5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pl5Cf5taG5t",
        "colab_type": "text"
      },
      "source": [
        "###Superponemos gráficos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKfIN0P3aJOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_f_h1_first, b_Pxx_h1_first = blackman_tukey(fs=fs, data=strain_h1, figure_number=1, x_lim=(10,2000), title='Strain H1', M=20,N=4096*2, frac_N_l=5)\n",
        "b_f_l1_first, b_Pxx_l1_first = blackman_tukey(fs=fs, data=strain_l1[:len(strain_l1)//3], figure_number=1, x_lim=(10,2000), title='Strain L1', M=4, N=4096*3, frac_N_l=16)\n",
        "plt.legend(['H1','L1'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E28PVKh0sWQH",
        "colab_type": "text"
      },
      "source": [
        "## c) Estimación del espectro de potencia del ruido mediante el uso del método de Welch-Bartlett"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDy6GfPOs1bZ",
        "colab_type": "text"
      },
      "source": [
        "### Implementación del método de Welch-Bartlett"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3KGkxM0DpPN",
        "colab_type": "text"
      },
      "source": [
        "En la implementación de Welch Bartlett, se utiliza una ventana de tipo Hanning para reducir el bias. Probándose con una ventana cuadrada, se observó un bias notable (las frecuencias altas se veían atenuadas con respecto al espectro correcto)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ay68e6oXak7k",
        "colab": {}
      },
      "source": [
        "#Inspirado en https://stackoverflow.com/questions/46882312/python-implementation-of-bartlett-periodogram\n",
        "\n",
        "#nperseg : L\n",
        "def welch_barlett(data, fs,  D=0, nperseg=512, nfft=512, title='',x_lim=(10,2000), plot=True):\n",
        "    if nfft % 2:\n",
        "      numFreqs = (nfft + 1)//2\n",
        "    else:\n",
        "      numFreqs = nfft//2 + 1\n",
        "      \n",
        "    x = mlab.stride_windows(data, nfft, 0, axis=0)\n",
        "    window = np.ones(nfft, x.dtype)\n",
        "    window = np.hanning(len(np.ones(nfft, x.dtype)))*np.ones(nfft, x.dtype)\n",
        "    x = x * window.reshape((-1, 1))\n",
        "    x = np.fft.fft(x, n=nfft, axis=0)[:numFreqs, :]\n",
        "    freqs = np.fft.fftfreq(nfft, 1/fs)[:numFreqs]\n",
        "    freqs[len(freqs)-1] = -freqs[len(freqs)-1]\n",
        "    x = np.conj(x) * x\n",
        "    \n",
        "    x /= fs\n",
        "    x /= (np.abs(window)**2).sum()\n",
        "    x = np.transpose(x).sum(axis=0)/len(x[1])\n",
        "    if plot: \n",
        "      plt.loglog(freqs, x)\n",
        "      plt.xlim(x_lim)\n",
        "      plt.grid()\n",
        "      plt.title(title)\n",
        "      plt.xlabel(\"Frequency (Hz)\")\n",
        "      plt.ylabel(\"Power Spectral Density Rx(f)\")    \n",
        "    \n",
        "    return freqs, x \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pb1Kv37s_6m",
        "colab_type": "text"
      },
      "source": [
        "### Aplicado a los datos de Livingston"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7gZ-8QpmHsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nperseg = 4096\n",
        "nfft = nperseg\n",
        "w_f_l1_tot, w_Pxx_l1_tot = welch_barlett(data=strain_l1, fs=fs, nperseg=nperseg, nfft=nfft, title='Welch-Barlett L1',x_lim=(10,2000), plot=True)\n",
        "plt.show()\n",
        "w_f_l1_first, w_Pxx_l1_first = welch_barlett(data=strain_l1[:len(strain_l1)//3], fs=fs, nperseg=nperseg, nfft=nfft, title='Welch-Barlett L1',x_lim=(10,2000), plot=True)\n",
        "plt.show()\n",
        "w_f_l1_last, w_Pxx_l1_last = welch_barlett(data=strain_l1[2*len(strain_l1)//3:], fs=fs, nperseg=nperseg, nfft=nfft, title='Welch-Barlett L1',x_lim=(10,2000), plot=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS2ZULjmtZOB",
        "colab_type": "text"
      },
      "source": [
        "### Aplicado a los datos de Hanford"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaH9qvBltdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w_f_h1_tot, w_Pxx_h1_tot = welch_barlett(data=strain_h1, fs=fs, nperseg=nperseg, nfft=nfft, title='Welch-Barlett H1',x_lim=(10,2000), plot=True)\n",
        "plt.show()\n",
        "w_f_h1_first, w_Pxx_h1_first = welch_barlett(data=strain_h1[:len(strain_h1)//3], fs=fs, nperseg=nperseg, nfft=nfft, title='Welch-Barlett H1',x_lim=(10,2000), plot=True)\n",
        "plt.show()\n",
        "w_f_h1_last, w_Pxx_h1_last = welch_barlett(data=strain_h1[2*len(strain_h1)//3:], fs=fs, nperseg=nperseg, nfft=nfft, title='Welch-Barlett H1',x_lim=(10,2000), plot=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQpzI4EkaWX1",
        "colab_type": "text"
      },
      "source": [
        "###Superponemos gráficos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4O9kn3PaXOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w_f_h1_first, w_Pxx_h1_first = welch_barlett(data=strain_h1[:len(strain_h1)//3], fs=fs, nperseg=nperseg, nfft=nfft, title='Welch-Barlett H1',x_lim=(10,2000), plot=True)\n",
        "w_f_l1_first, w_Pxx_l1_first = welch_barlett(data=strain_l1[:len(strain_l1)//3], fs=fs, nperseg=nperseg, nfft=nfft, title='Welch-Barlett L1',x_lim=(10,2000), plot=True)\n",
        "plt.legend(['H1','L1'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_shLL4CtsEs",
        "colab_type": "text"
      },
      "source": [
        "## d) Estimación del espectro de potencia del ruido mediante el uso del enfoque multitaper con funciones DPSS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A78UcFLhuDQd",
        "colab_type": "text"
      },
      "source": [
        "### Implementación del método multitaper con funciones DPSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld6Nmq-XuB9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# para la matemática:\n",
        "# https://ccrma.stanford.edu/~jos/sasp/Slepian_DPSS_Window.html\n",
        "# para las ventanas:\n",
        "# https://scipy.github.io/devdocs/generated/scipy.signal.windows.dpss.html\n",
        "def multitaper_dpss(data, fs, windows, x_lim=(10,2000), title='', plot=True):\n",
        "  # tomo a x_lim[1] como el bandwith!!!\n",
        "  R_i = abs( np.fft.fft( a=np.multiply(data,windows) ) ) ** 2 / len(data)\n",
        "  freqs = np.fft.fftfreq( n=len(data), d=1/fs )\n",
        "  R = np.sum(R_i, axis=0) / len(windows)\n",
        "  if plot: \n",
        "    plt.loglog(freqs, 2/len(data) * R)\n",
        "    plt.xlim(x_lim)\n",
        "    plt.grid()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Frequency (Hz)\")\n",
        "    plt.ylabel(\"Power Spectral Density Rx(f)\")\n",
        "  return freqs, R"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8baz5b1uTrl",
        "colab_type": "text"
      },
      "source": [
        "### Aplicado a los datos de Livingston"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXb8PURitKcU",
        "colab_type": "text"
      },
      "source": [
        "Preparamos las ventanas slepianas con las que vamos a trabajar sobre los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuooBhlruBxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sym bool, optional\n",
        "# When True (default), generates a symmetric window, for use in filter design. When False, generates a periodic window, for use in spectral analysis.\n",
        "\n",
        "#tomamos bandwith = 2e3\n",
        "sequences = scsig.windows.dpss(M=len(strain_l1), NW=4096//2, Kmax=4, sym=False)\n",
        "sequences_short = scsig.windows.dpss(M=len(strain_l1)//3, NW=4096//2, Kmax=4, sym=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNQAv2i-uUOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_f_l1_tot, f_Pxx_l1_tot = multitaper_dpss(data=strain_l1, fs=fs, windows=sequences, x_lim=(10,2000), title='Multitaper L1', plot=True)\n",
        "plt.show()\n",
        "d_f_l1_first, f_Pxx_l1_first = multitaper_dpss(data=strain_l1[:len(strain_l1)//3], fs=fs, windows=sequences_short, x_lim=(10,2000), title='Multitaper L1, primeros 10 segundos', plot=True)\n",
        "plt.show()\n",
        "d_f_l1_last, f_Pxx_l1_last = multitaper_dpss(data=strain_l1[2*len(strain_l1)//3+1:], fs=fs, windows=sequences_short, x_lim=(10,2000), title='Multitaper L1, últimos 10 segundos', plot=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW_q5dI7uVRu",
        "colab_type": "text"
      },
      "source": [
        "### Aplicado a los datos de Hanford"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kaz2xF91uUDn"
      },
      "source": [
        "Preparamos las ventanas slepianas con las que vamos a trabajar sobre los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qrnS0CrduKRE",
        "colab": {}
      },
      "source": [
        "# sym bool, optional\n",
        "# When True (default), generates a symmetric window, for use in filter design. When False, generates a periodic window, for use in spectral analysis.\n",
        "\n",
        "#tomamos bandwith = 2e3\n",
        "sequences = scsig.windows.dpss(M=len(strain_h1), NW=4096//2, Kmax=6, sym=False)\n",
        "sequences_short = scsig.windows.dpss(M=len(strain_h1)//3, NW=4096//2, Kmax=7, sym=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7nDEqmBuV2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_f_h1_tot, d_Pxx_h1_tot = multitaper_dpss(data=strain_h1, fs=fs, windows=sequences, x_lim=(10,2000), title='Multitaper H1', plot=True)\n",
        "plt.show()\n",
        "d_f_h1_first, d_Pxx_h1_first = multitaper_dpss(data=strain_h1[:len(strain_h1)//3], fs=fs, windows=sequences_short, x_lim=(10,2000), title='Multitaper H1, primeros 10 segundos', plot=True)\n",
        "plt.show()\n",
        "d_f_h1_last, d_Pxx_h1_last = multitaper_dpss(data=strain_h1[2*len(strain_h1)//3+1:], fs=fs, windows=sequences_short, x_lim=(10,2000), title='Multitaper H1, últimos 10 segundos', plot=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQmAIZbYa58g",
        "colab_type": "text"
      },
      "source": [
        "###Superponemos gráficos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rybjwBVna5g4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_f_h1_first, d_Pxx_h1_first = multitaper_dpss(data=strain_h1[:len(strain_h1)//3], fs=fs, windows=sequences_short, x_lim=(10,2000), title='Multitaper H1, primeros 10 segundos', plot=True)\n",
        "d_f_l1_first, f_Pxx_l1_first = multitaper_dpss(data=strain_l1[:len(strain_l1)//3], fs=fs, windows=sequences_short, x_lim=(10,2000), title='Multitaper L1, primeros 10 segundos', plot=True)\n",
        "plt.legend(['H1','L1'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbWQODJwWfB5",
        "colab_type": "text"
      },
      "source": [
        "La mejor estimación fue obtenida con el método de Welch-Bartlett, además fue sencillo encontrar parámetros que se ajustaran al espectro. Por otro lado, la obtenida con Blackman-Tukey fue poco satisfactoria ya que se notó la presencia de bias, posiblemente debido a la ventana elegida. El periodograma, por su parte, fue sencillo de implementar, pero la alta varianza de la estimación lo hace un método no muy utilizado, ya que es preferible usar Welch-Bartlett. Por último, con la estimación multitaper se obtuvieron resultados aceptables, sacrificando resolución espectral, debido a las ventanas usadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSyIIk8POOVh",
        "colab_type": "text"
      },
      "source": [
        "#Ejercicio 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkL8SIKyOQjT",
        "colab_type": "text"
      },
      "source": [
        "## Inversa del espectro de potencia del ruido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9CfHnmZOgo4",
        "colab_type": "text"
      },
      "source": [
        "### Implementación el filtro inversor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crSg-6dXOfji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "psd_l1 = interp1d(w_f_l1_tot, w_Pxx_l1_tot)\n",
        "psd_h1 = interp1d(w_f_h1_tot, w_Pxx_h1_tot)\n",
        "                  \n",
        "def whiten(strain, interp_psd, dt):\n",
        "    Nt = len(strain)\n",
        "    freqs = np.fft.rfftfreq(Nt, dt)\n",
        "    hf = np.fft.rfft(strain)\n",
        "    white_hf = hf / np.sqrt(interp_psd(freqs) /dt/2.)\n",
        "    white_ht = np.fft.irfft(white_hf, n=Nt)\n",
        "    return white_ht\n",
        "\n",
        "strain_h1_whiten = whiten(strain_h1, psd_h1, dt_h1)\n",
        "strain_l1_whiten = whiten(strain_l1, psd_l1, dt_l1)\n",
        "bb, ab = scsig.butter(4, [20*2./fs, 300*2./fs], btype='band')\n",
        "strain_H1_whitenbp = scsig.filtfilt(bb, ab, strain_h1_whiten)\n",
        "strain_L1_whitenbp = scsig.filtfilt(bb, ab, strain_l1_whiten)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NfDqSOIOtCp",
        "colab_type": "text"
      },
      "source": [
        "## Señal gravitatoria en el momento del evento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwHNLjeVl5KO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strain_L1_shift = -np.roll(strain_L1_whitenbp, int(0.007*fs))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(66750,67500)*dt_l1, strain_L1_shift[66750:67500],'g',label='L1 strain')\n",
        "plt.plot(np.arange(66750,67500)*dt_h1, strain_H1_whitenbp[66750:67500],'r',label='H1 strain')\n",
        "plt.xlabel('Tiempo t')\n",
        "plt.ylabel('Señal a la salida del filtro ')\n",
        "plt.legend(loc='lower left')\n",
        "plt.title('Señal gravitatoria en el momento del evento')\n",
        "\n",
        "\n",
        "from IPython.display import Audio\n",
        "Audio(strain_L1_whitenbp[66750:67500],rate=4096)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kQngq0xSLS6",
        "colab_type": "text"
      },
      "source": [
        "## Buscamos el time shift correspondiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBlcR2QpSPXT",
        "colab_type": "text"
      },
      "source": [
        "Debido a que estamos trabajando con ondas cuyas velocidades corresponden a velocidades relativistas, sabemos que los eventos no sucederán simultáneamente en cada sistema. \n",
        "Buscamos encontrar entonces el corrimiento en tiempo que deberemos realizar en los datos de uno de los dos sistemas para poder verificar la correspondencia entre eventos.\n",
        "Para solucionar este problema, hallamos las correlaciones cruzadas entre los datos de Hanford y los de Livingston. \n",
        "Pondremos una cota para los tiempos de shifting que podremos encontrar para así reducir el tiempo de cálculo.\n",
        "Dado que la distancia entre Hanford y Livingston es de  3030.13 km, (https://astronomy.stackexchange.com/questions/13704/gravitational-wave-detection-time-difference-between-ligo-livingston-and-ligo-ha) y la velocidad de la luz es de 299792458 (km/s), entonces el máximo shifting en tiempo que se podrá tener entre las dos señales es de 10(ms). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpc9a2aI8ZKs",
        "colab_type": "text"
      },
      "source": [
        "### Implementación de la autocorrelación y obtención del time shift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaSxeK6C6Tm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stackoverflow.com/a/4688875\n",
        "def get_time_shift(data1, data2):\n",
        "  time_shift = np.argmax(np.abs(scsig.correlate(data1,data2, 'same')[:int(10e-3/dt_l1)]))\n",
        "  return time_shift\n",
        "\n",
        "time_shift = get_time_shift(strain_L1_whitenbp[:len(strain_L1_whitenbp)//3], strain_H1_whitenbp[:len(strain_H1_whitenbp)//3])\n",
        "print('Time shift: %0.2f milisegundos' % float(time_shift*dt_l1*1e3) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IPQN7FfuQLGq"
      },
      "source": [
        "#Ejercicio 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YmGsoyDxQUDQ"
      },
      "source": [
        "## Mejora de la estimación del espectro de potencia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rltb5ZF1QsRT",
        "colab_type": "text"
      },
      "source": [
        "Según lo que menciona el paper https://arxiv.org/pdf/1604.06211.pdf, tanto la señal de Livingston $S_L$, como la de Hanford $S_H$ se pueden expresar como:\n",
        "\n",
        "$S_L(t) = G(t) + n_L(t) + F_L(t)$\n",
        "\n",
        "$S_H = (±)G(t ± τ ) + n_H(t) + F_H(t)$\n",
        "\n",
        "con $G(t)$ el evento observado en el detector $L$ y observado, (posiblemente invertido) en el detector $H$ en un tiempo shifteado por $±τ$. Además, $n_H(t)$, $n_L(t)$ son las componentes de ruido de banda ancha y $F_L(t)$, $F_H(t)$ se refieren a los efectos de banda angosta que pueden ser tiempo-dependientes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fgwphBcYRBZ",
        "colab_type": "text"
      },
      "source": [
        "###Filtramos con un pasabanda las frecuencias mayores a 300Hz y menores a 30Hz, y suavizamos picos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DinfzSVDMzjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bandpaseamos la señal entre 30-300 Hz\n",
        "bb, ab = scsig.butter(4, [30.*2./fs, 300.*2./fs], btype='band')\n",
        "strain_H1_bp = scsig.filtfilt(bb, ab, strain_H1)\n",
        "strain_L1_bp = scsig.filtfilt(bb, ab, strain_L1)\n",
        "\n",
        "fft_H1 = np.fft.fft(strain_H1_bp)\n",
        "fft_L1 = np.fft.fft(strain_L1_bp)\n",
        "\n",
        "#buscamos picos\n",
        "peaks_H1 = scsig.find_peaks(fft_H1)\n",
        "peaks_L1 = scsig.find_peaks(fft_L1)\n",
        "\n",
        "#eliminamos picos en base a los encontrados\n",
        "for i in range(len(peaks_H1)):\n",
        "    fft_L1[i]=0\n",
        "    fft_H1[i]=0\n",
        "\n",
        "#recuperamos señal\n",
        "sig_L1 = np.real(np.fft.ifft(fft_L1))\n",
        "sig_H1 = np.real(np.fft.ifft(fft_H1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6yz2DnBVgGv",
        "colab_type": "text"
      },
      "source": [
        "###Calculamos máxima autocorrelación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa25UDxUNgPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#matriz de autocorrelación (función creada por Santiago Ivulich)\n",
        "\n",
        "N = len(sig_L1)\n",
        "L = int(np.floor(200*1e-3*fs))\n",
        "K = int(np.floor((N-1)*2./L+1))\n",
        "D = L//2\n",
        "segments_H1 = [sig_H1[i*D:i*D+L] for i in range(K)]\n",
        "segments_L1 = [sig_L1[i*D:i*D+L] for i in range(K)]\n",
        "\n",
        "MAX_CORR = 0\n",
        "MAX_POS = [0,0]\n",
        "MAX_TAU = 0\n",
        "for i,seg_H in enumerate(segments_H1):\n",
        "    for j,seg_L in enumerate(segments_L1):\n",
        "        if(i>10 and j>10 and i<K-10 and j<K-10):\n",
        "            corr_ij = np.correlate(seg_H, seg_L,\"same\")\n",
        "            corr_ij_max = np.amax(np.abs(corr_ij))\n",
        "            indx = np.where(np.abs(corr_ij)==corr_ij_max)\n",
        "            if(corr_ij_max > np.abs(MAX_CORR)):\n",
        "                MAX_CORR = corr_ij[indx[0][0]]\n",
        "                MAX_POS = [i,j]\n",
        "                MAX_TAU = (indx[0][0]-len(corr_ij)/2)/fs\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(np.linspace(-0.1,0.1,len(segments_H1[MAX_POS[0]])),segments_H1[MAX_POS[0]],label='H1 strain')\n",
        "plt.plot(np.linspace(-0.1,0.1,len(segments_H1[MAX_POS[0]])),np.sign(MAX_CORR)*np.roll(segments_L1[MAX_POS[1]],int(MAX_TAU*fs)),label='L1 strain')\n",
        "plt.xlim((-0.1,0.1))\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n",
        "print('Max correlation with a diference of {}ms'.format(MAX_TAU*1000))\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ-UEsA4Vz31",
        "colab_type": "text"
      },
      "source": [
        "El error puede deberse al método utilizado para suavizar los picos."
      ]
    }
  ]
}